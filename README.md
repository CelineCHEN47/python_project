# Spam Email Analysis & Reply Generation Project

## Project Overview
This project analyzes emails for spam detection and generates intelligent replies using fine-tuned LLMs. It includes both backend AI models and a user-friendly Streamlit WebUI.

## Project Structure

### Backend Components
- **preprocessing.py**: Preprocesses the DailyDialog dataset into email-style prompt-response pairs
- **train.py**: Fine-tunes GPT-2/DistilGPT-2 with LoRA for email reply generation

### How to call the generation model:
As i have used the Lora, so the gpt2 model can not be directly called, it should be merged with the Lora.

How to understand the folder: 
âœ…Folder 1: Contains `adapter_model.safetensors` and `adapter_config.json`
a standard LoRA adapter directory. It only stores the LoRA weights and configurationâ€”not the base GPT-2 model.

`adapter_model.safetensors`: LoRA fine-tuned weights.
`adapter_config.json:` LoRA hyperparameters (e.g., rank, alpha).
Tokenizer files (`tokenizer.json`, `vocab.json`, `merges.txt`, etc.).

âœ… Folder 2: Contains `model.safetensors` and a subfolder `lora_adapter/`
The files inside should be the main body of our trained model.

`model.safetensors`: Likely the base GPT-2 model weights.
`lora_adapter/`: Contains the LoRA adapter files.
Other config and tokenizer files.

#### Way to call it
I directly share `LoRA Adapter` + `Base Model`, this way is more flexible. (Such as the folder "gpt2(300)")

``` 
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load base model
base_model_name = "gpt2"  # Or you can load the model, tokenizer from Folder 2.
tokenizer = AutoTokenizer.from_pretrained(base_model_name)
model = AutoModelForCausalLM.from_pretrained(base_model_name)

# Load LoRA adapter  (the key difference from directly call a model)
lora_path = "./lora_adapter"  # Path to Folder 1
merged_model = PeftModel.from_pretrained(model, lora_path)

# Generate text
input_text = "Hello, how are you?"
inputs = tokenizer(input_text, return_tensors="pt")
outputs = merged_model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```


### Frontend
- **app.py**: Streamlit WebUI for email analysis and reply generation (NEW)

### Data
- **data/email_reply_dataset/**: Preprocessed training data for LLM

## Setup & Usage

### 1. Data Preprocessing
```bash
python preprocessing.py \
  --dataset_name kmyoo/dailydialog-tiny \
  --output_dir ./data/email_reply_dataset \
  --style support
```
This saves the processed dataset to `data/email_reply_dataset`.

### 2. Model Training
```bash
# Full parameter fine-tuning
python train.py --model_name gpt2

# With LoRA (recommended)
python train.py --model_name gpt2 --use_lora
```

The current code ignores evaluation to ensure it runs locally. Results are saved to `./results/`.

### 3. Running the WebUI (NEW - v0.1)
```bash
pip install -r requirements.txt
streamlit run app.py
```

## Recent Updates (v0.1)

### New: Streamlit WebUI (`app.py`)
A fully functional web interface with:

#### Features:
- **ðŸ” Email Analysis Tab**: Analyze emails for spam detection and reply generation
- **ðŸ“‹ History Tab**: View past analyses with timestamps and generated replies
- **âš™ï¸ Settings Tab**: Configure models, thresholds, and advanced options

#### Current Implementation:
- **Mock Spam Classifier**: Random classification (70-99% confidence) - ready for model integration
- **Mock LLM Generator**: Generic predetermined responses - ready for actual model integration
- Session state management for analysis history
- Professional UI with color-coded badges and styling
- Copy/download functionality for generated replies

#### Integration Points:
To integrate your actual models:
1. Replace `MockSpamClassifier.predict()` in `app.py` with your trained spam detection model
2. Replace `MockLLMReplyGenerator.generate_reply()` with your fine-tuned GPT-2/DistilGPT-2
3. Update model loading logic in the Settings tab

### New: Requirements File (`requirements.txt`)
Added comprehensive dependencies list for easy setup.

## Dependencies
- streamlit >= 1.28.0
- torch >= 2.0.0
- transformers >= 4.30.0
- datasets >= 2.13.0
- peft >= 0.4.0
- evaluate >= 0.4.0
- numpy >= 1.24.0

## Notes
- Result folders generated by `train.py` have been manually deleted to keep the repository clean
- The preprocessing and training code ignores evaluation metrics to ensure local execution
- The WebUI skeleton is ready for production-ready model integration
