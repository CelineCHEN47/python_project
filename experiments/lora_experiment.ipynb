{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13834204,"sourceType":"datasetVersion","datasetId":8810607},{"sourceId":13834210,"sourceType":"datasetVersion","datasetId":8810612}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-23T08:29:29.109563Z","iopub.execute_input":"2025-11-23T08:29:29.110169Z","iopub.status.idle":"2025-11-23T08:29:29.374058Z","shell.execute_reply.started":"2025-11-23T08:29:29.110144Z","shell.execute_reply":"2025-11-23T08:29:29.373290Z"}},"outputs":[{"name":"stdout","text":"Sun Nov 23 08:29:29 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   35C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   33C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T08:43:25.504538Z","iopub.execute_input":"2025-11-23T08:43:25.504810Z","iopub.status.idle":"2025-11-23T08:43:35.895533Z","shell.execute_reply.started":"2025-11-23T08:43:25.504790Z","shell.execute_reply":"2025-11-23T08:43:35.894413Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.4.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.5)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.18)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.10.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\nCollecting pyarrow>=21.0.0 (from datasets>=2.0.0->evaluate)\n  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (0.28.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.10.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.11.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1)\nDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyarrow, evaluate\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 19.0.1\n    Uninstalling pyarrow-19.0.1:\n      Successfully uninstalled pyarrow-19.0.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.6 pyarrow-22.0.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\n\n# Êü•ÁúãËæìÂÖ•ÁõÆÂΩï‰∏ãÁöÑÂÜÖÂÆπ\nprint(\"Input directory:\", os.listdir('/kaggle/input'))\n\n# ÈÅçÂéÜ input ÁõÆÂΩï\nfor root, dirs, files in os.walk('/kaggle/input', topdown=True):\n    print(root, dirs, files)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T06:19:59.039342Z","iopub.execute_input":"2025-11-23T06:19:59.039618Z","iopub.status.idle":"2025-11-23T06:19:59.045118Z","shell.execute_reply.started":"2025-11-23T06:19:59.039597Z","shell.execute_reply":"2025-11-23T06:19:59.044383Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Input directory: ['.virtual_documents']\n/kaggle/working ['.virtual_documents'] []\n/kaggle/working/.virtual_documents [] []\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom datasets import Dataset, load_from_disk\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport numpy as np\nimport evaluate\nimport subprocess\nimport psutil\nimport os\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n\n# ======================\n# ÂÜÖÂ≠òÁõëÊéßÂáΩÊï∞\n# ======================\ndef print_memory_usage(tag=\"\"):\n    print(f\"\\n===== Memory Usage {tag} =====\")\n    try:\n        gpu_info = subprocess.check_output(\n            [\"nvidia-smi\", \"--query-gpu=memory.used,memory.total\", \"--format=csv,noheader,nounits\"]\n        )\n        gpu_info = gpu_info.decode(\"utf-8\").strip().split(\"\\n\")\n        for i, gpu in enumerate(gpu_info):\n            used, total = map(int, gpu.split(','))\n            print(f\"GPU {i}: {used}MB / {total}MB\")\n    except:\n        print(\"No GPU found\")\n\n    ram = psutil.virtual_memory()\n    print(f\"CPU RAM: {ram.used/1024**2:.2f}MB / {ram.total/1024**2:.2f}MB\")\n    print(\"====================================\\n\")\n\n\n# ======================\n# Trainer ÂõûË∞ÉÁ±ª\n# ======================\nfrom transformers import TrainerCallback\n\nclass MemoryCallback(TrainerCallback):\n    def on_step_end(self, args, state, control, **kwargs):\n        if state.global_step % 100 == 0 and state.global_step > 0:\n            print_memory_usage(f\"Training step {state.global_step}\")\n\n\n# ==========================================================\n# ============ 0. Ê£ÄÊü•ÊòØÂê¶Â∑≤Êúâ tokenized ÁºìÂ≠ò ================\n# ==========================================================\ncache_path = \"/tokenized_enron_spam\"\n\nif os.path.exists(cache_path):\n    print(\"üîÑ Loading cached tokenized dataset...\")\n    tokenized_datasets = load_from_disk(cache_path)\n\nelse:\n    print(\"üÜï No cache found ‚Äî processing raw dataset...\")\n\n    # ======================\n    # 1. Load Dataset\n    # ======================\n    df = pd.read_csv(\"/kaggle/input/classification-data/enron_spam_data.csv\")\n    dataset = Dataset.from_pandas(df)\n    dataset = dataset.train_test_split(test_size=0.2)\n\n    # ======================\n    # 2. Preprocess Data\n    # ======================\n    model_name = \"distilbert-base-uncased\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    def prepare_dataset(example):\n        example[\"label\"] = 1 if example[\"Spam/Ham\"].lower() == \"spam\" else 0\n        return example\n\n    dataset = dataset.map(prepare_dataset)\n\n    def preprocess_function(examples):\n        messages = [msg if msg is not None else \"\" for msg in examples[\"Message\"]]\n        return tokenizer(messages, truncation=True, padding=\"max_length\")\n\n    dataset = dataset.filter(lambda x: x[\"Message\"] is not None and len(str(x[\"Message\"]).strip()) > 0)\n\n    tokenized_datasets = dataset.map(preprocess_function, batched=True)\n\n    # ‰øùÂ≠òÁºìÂ≠ò\n    print(\"üíæ Saving tokenized dataset...\")\n    tokenized_datasets.save_to_disk(cache_path)\n    print(f\"‚úî Tokenized dataset saved to {cache_path}\")\n\n# tokenizer ÂøÖÈ°ªÂú®Â§ñÈÉ®ÂàõÂª∫‰∏ÄÊ¨°\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\n# Label map\nid2label = {0: \"HAM\", 1: \"SPAM\"}\nlabel2id = {\"HAM\": 0, \"SPAM\": 1}\n\n# ======================\n# 3. Load Model\n# ======================\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\",\n    num_labels=2,\n    id2label=id2label,\n    label2id=label2id\n)\n\n# ======================\n# 4. Metrics\n# ======================\nmetric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\n\n# ======================\n# 5. Training Arguments\n# ======================\ntraining_args = TrainingArguments(\n    output_dir=\"kagglespam_classification_results\",\n    learning_rate=2e-5,\n    dataloader_num_workers=0,\n    per_device_train_batch_size=64,\n    per_device_eval_batch_size=64,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)\n\n# ======================\n# 6. Trainer\n# ======================\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n    callbacks=[MemoryCallback()],\n)\n\n# ======================\n# 7. Train\n# ======================\nprint_memory_usage(\"Before training\")\nprint(\"Starting training...\")\ntrainer.train()\nprint_memory_usage(\"After training\")\n\n# ======================\n# 8. Evaluate\n# ======================\nprint(\"Evaluating...\")\neval_results = trainer.evaluate()\nprint(f\"Evaluation results: {eval_results}\")\n\n# ======================\n# 9. Save Model\n# ======================\ntrainer.save_model(\"spam_classifier_model__full\")\nprint(\"Model saved to spam_classifier_model__full\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T08:43:38.204131Z","iopub.execute_input":"2025-11-23T08:43:38.204643Z","iopub.status.idle":"2025-11-23T09:09:53.735472Z","shell.execute_reply.started":"2025-11-23T08:43:38.204611Z","shell.execute_reply":"2025-11-23T09:09:53.734814Z"}},"outputs":[{"name":"stderr","text":"2025-11-23 08:43:59.951971: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763887440.336696      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763887440.478062      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"üÜï No cache found ‚Äî processing raw dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b6bbc3fdb424f468e0abbe001679410"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"310d062216294d8b98d20e45a8666f24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52cc182499614df2911c602d55a4d04f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07fd9c470ed24713ad4ef175c58727bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/26972 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c11df9b559f34da3b70a67ec52d36d9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6744 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"736eadbaf7c349c283b957dc25bd8482"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/26972 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12534b16683c487c888109662e0ab0d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/6744 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dca2b41436d4487ae1a4f58b932ccd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/26671 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02a1e59c01604f0189a6305907a038a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6674 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06d3f599490e4ac1b25a7899c7e3da1d"}},"metadata":{}},{"name":"stdout","text":"üíæ Saving tokenized dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/26671 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6510f1c3a254e178843bc09ec5f2347"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/6674 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3836e7553664b41a88563272003b1e2"}},"metadata":{}},{"name":"stdout","text":"‚úî Tokenized dataset saved to /tokenized_enron_spam\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d2dbd2f093b4b5987b9654140ce9f84"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3db9f45a15a4e5e9ab125b5fecca622"}},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_48/1982602989.py:137: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"\n===== Memory Usage Before training =====\nGPU 0: 395MB / 15360MB\nGPU 1: 3MB / 15360MB\nCPU RAM: 2728.25MB / 32102.90MB\n====================================\n\nStarting training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='418' max='418' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [418/418 23:48, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.034197</td>\n      <td>0.987863</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.030358</td>\n      <td>0.990261</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\n===== Memory Usage Training step 100 =====\nGPU 0: 11411MB / 15360MB\nGPU 1: 10621MB / 15360MB\nCPU RAM: 3498.99MB / 32102.90MB\n====================================\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n===== Memory Usage Training step 200 =====\nGPU 0: 11411MB / 15360MB\nGPU 1: 10621MB / 15360MB\nCPU RAM: 3475.33MB / 32102.90MB\n====================================\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n===== Memory Usage Training step 300 =====\nGPU 0: 11411MB / 15360MB\nGPU 1: 10621MB / 15360MB\nCPU RAM: 3513.51MB / 32102.90MB\n====================================\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n===== Memory Usage Training step 400 =====\nGPU 0: 11411MB / 15360MB\nGPU 1: 10621MB / 15360MB\nCPU RAM: 3506.30MB / 32102.90MB\n====================================\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n===== Memory Usage After training =====\nGPU 0: 11411MB / 15360MB\nGPU 1: 10621MB / 15360MB\nCPU RAM: 3494.85MB / 32102.90MB\n====================================\n\nEvaluating...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [53/53 00:56]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation results: {'eval_loss': 0.030358023941516876, 'eval_accuracy': 0.9902607132154629, 'eval_runtime': 58.0805, 'eval_samples_per_second': 114.91, 'eval_steps_per_second': 0.913, 'epoch': 2.0}\nModel saved to spam_classifier_model__full\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport subprocess\nimport psutil\nimport numpy as np\nimport pandas as pd\n\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    TrainerCallback\n)\nfrom datasets import Dataset, load_from_disk\nfrom peft import LoraConfig, get_peft_model\nimport evaluate  # ‰ΩøÁî® import evaluate ËÄåÈùû from evaluate import load\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# ===================================\n# ÂÜÖÂ≠òÁõëÊéßÂáΩÊï∞\n# ===================================\ndef print_memory_usage(tag=\"\"):\n    print(f\"\\n===== Memory Usage {tag} =====\")\n    try:\n        gpu_info = subprocess.check_output(\n            [\"nvidia-smi\", \"--query-gpu=memory.used,memory.total\", \"--format=csv,noheader,nounits\"]\n        )\n        gpu_info = gpu_info.decode(\"utf-8\").strip().split(\"\\n\")\n        for i, gpu in enumerate(gpu_info):\n            used, total = map(int, gpu.split(','))\n            print(f\"GPU {i}: {used}MB / {total}MB\")\n    except Exception as e:\n        print(\"No GPU found or nvidia-smi not available\")\n\n    ram = psutil.virtual_memory()\n    print(f\"CPU RAM: {ram.used/1024**2:.2f}MB / {ram.total/1024**2:.2f}MB\")\n    print(\"====================================\\n\")\n\n\n# ===================================\n# Trainer ÊòæÂ≠òÁõëÊéß Callback\n# ===================================\nclass MemoryCallback(TrainerCallback):\n    def on_step_end(self, args, state, control, **kwargs):\n        if state.global_step % 100 == 0 and state.global_step > 0:\n            print_memory_usage(f\"Training step {state.global_step}\")\n\n\n# ==========================================================\n# ============ 0. Ê£ÄÊü•ÊòØÂê¶Â∑≤Êúâ tokenized ÁºìÂ≠ò ================\n# ==========================================================\ncache_path = \"/tokenized_enron_spam\"\n\nif os.path.exists(cache_path):\n    print(\"üîÑ Loading cached tokenized dataset...\")\n    tokenized_datasets = load_from_disk(cache_path)\nelse:\n    print(\"üÜï No cache found ‚Äî processing raw dataset...\")\n\n    # -----------------------\n    # 1. Load raw CSV\n    # -----------------------\n    df = pd.read_csv(\"/kaggle/input/classification-data/enron_spam_data.csv\")\n    dataset = Dataset.from_pandas(df)\n    dataset = dataset.train_test_split(test_size=0.2)\n\n    # -----------------------\n    # 2. Preprocess labels & text\n    # -----------------------\n    model_name = \"distilbert-base-uncased\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    def prepare_dataset(example):\n        example[\"label\"] = 1 if example[\"Spam/Ham\"].lower() == \"spam\" else 0\n        return example\n\n    dataset = dataset.map(prepare_dataset)\n\n    def preprocess_function(examples):\n        messages = [msg if msg is not None else \"\" for msg in examples[\"Message\"]]\n        return tokenizer(messages, truncation=True, padding=\"max_length\", max_length=512)\n\n    # Filter out empty messages\n    dataset = dataset.filter(lambda x: x[\"Message\"] is not None and len(str(x[\"Message\"]).strip()) > 0)\n\n    tokenized_datasets = dataset.map(preprocess_function, batched=True, batch_size=1000)\n\n    # Save cache\n    print(\"üíæ Saving tokenized dataset...\")\n    tokenized_datasets.save_to_disk(cache_path)\n    print(f\"‚úî Tokenized dataset saved to {cache_path}\")\n\n# Re-initialize tokenizer (ensure consistent instance)\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\nid2label = {0: \"HAM\", 1: \"SPAM\"}\nlabel2id = {\"HAM\": 0, \"SPAM\": 1}\n\n\n# ===================================\n# 2. Load Model with FP16 + LoRA\n# ===================================\nprint(\"Loading DistilBERT for LoRA...\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\",\n    num_labels=2,\n    id2label=id2label,\n    label2id=label2id,\n    torch_dtype=torch.float16,  # FP16 for memory efficiency\n)\n\n# ===================================\n# 3. LoRA config\n# ===================================\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"],  # DistilBERT specific\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"SEQ_CLS\",\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\n\n# ===================================\n# 4. Metrics\n# ===================================\nmetric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\n\n# ===================================\n# 5. Training Args\n# ===================================\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/spam_classification_LoRA_result\",\n    learning_rate=1e-4,\n    dataloader_num_workers=0,  # Avoid tokenizers fork warning\n    per_device_train_batch_size=64,\n    per_device_eval_batch_size=64,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    max_grad_norm=1.0,\n    report_to=\"none\",  # Suppress WANDB warning\n)\n\n\n# ===================================\n# 6. Trainer\n# ===================================\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n    callbacks=[MemoryCallback()],\n)\n\nprint_memory_usage(\"Before LoRA training\")\nprint(\"üöÄ Starting LoRA training...\")\ntrainer.train()\nprint_memory_usage(\"After LoRA training\")\n\n\n# ===================================\n# 7. Evaluation\n# ===================================\nprint(\"Evaluating...\")\neval_results = trainer.evaluate()\nprint(f\"Evaluation results: {eval_results}\")\n\n\n# ===================================\n# 8. Save LoRA adapter\n# ===================================\ntrainer.save_model(\"/kaggle/working/spam_classifier_LoRA_model\")\nprint(\"LoRA model saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T06:07:33.829013Z","iopub.execute_input":"2025-11-23T06:07:33.829653Z","iopub.status.idle":"2025-11-23T06:15:48.423100Z","shell.execute_reply.started":"2025-11-23T06:07:33.829621Z","shell.execute_reply":"2025-11-23T06:15:48.422220Z"},"jupyter":{"outputs_hidden":true,"source_hidden":true},"collapsed":true},"outputs":[{"name":"stderr","text":"2025-11-23 06:07:55.601300: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763878076.023848      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763878076.145067      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"üÜï No cache found ‚Äî processing raw dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a59652c10074c1a903dbd7bff600902"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b32a078af9bf4de187c80aa01070fada"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce6dd6a2cbd44b5f95d98507601baf7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32d54ed13f914964a5bad1864b6d85f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/26972 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c052a3dcc724346828619e1fbcf3883"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6744 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"543e7b3c2f0c46849ebeb2ff5cb3b135"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/26972 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec7939a436a94ec59e9ce34582a37a32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/6744 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4a11dc10f064b7a878ac6828b8374b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/26684 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8127ac65dca448c3bba069777643b0df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6661 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6d397b3e4084290b3c7af6382245278"}},"metadata":{}},{"name":"stdout","text":"üíæ Saving tokenized dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/26684 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db0323c3fb5c4bfe9c18c14504a83d73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/6661 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fec45b8c7384ea0ba48ff301875ab88"}},"metadata":{}},{"name":"stdout","text":"‚úî Tokenized dataset saved to /tokenized_enron_spam\nLoading DistilBERT for LoRA...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"346699eeadc0469c96f3fc4171128bd2"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 1,181,954 || all params: 68,136,964 || trainable%: 1.7347\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d05d4725dad422ba679327188372c05"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_48/2105563729.py:164: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"\n===== Memory Usage Before LoRA training =====\nGPU 0: 253MB / 15360MB\nGPU 1: 3MB / 15360MB\nCPU RAM: 2687.07MB / 32102.89MB\n====================================\n\nüöÄ Starting LoRA training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='418' max='418' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [418/418 06:08, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>nan</td>\n      <td>0.487614</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>nan</td>\n      <td>0.487614</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\n===== Memory Usage Training step 100 =====\nGPU 0: 6991MB / 15360MB\nGPU 1: 6989MB / 15360MB\nCPU RAM: 3518.64MB / 32102.89MB\n====================================\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n===== Memory Usage Training step 200 =====\nGPU 0: 6991MB / 15360MB\nGPU 1: 6989MB / 15360MB\nCPU RAM: 3517.01MB / 32102.89MB\n====================================\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:3177: RuntimeWarning: invalid value encountered in less\n  if operator(metric_value, self.state.best_metric):\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n===== Memory Usage Training step 300 =====\nGPU 0: 7017MB / 15360MB\nGPU 1: 7015MB / 15360MB\nCPU RAM: 3506.80MB / 32102.89MB\n====================================\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n===== Memory Usage Training step 400 =====\nGPU 0: 7017MB / 15360MB\nGPU 1: 7015MB / 15360MB\nCPU RAM: 3511.20MB / 32102.89MB\n====================================\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:3177: RuntimeWarning: invalid value encountered in less\n  if operator(metric_value, self.state.best_metric):\n","output_type":"stream"},{"name":"stdout","text":"\n===== Memory Usage After LoRA training =====\nGPU 0: 7017MB / 15360MB\nGPU 1: 7015MB / 15360MB\nCPU RAM: 3501.22MB / 32102.89MB\n====================================\n\nEvaluating...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [53/53 00:18]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation results: {'eval_loss': nan, 'eval_accuracy': 0.4876144723014562, 'eval_runtime': 18.5303, 'eval_samples_per_second': 359.464, 'eval_steps_per_second': 2.86, 'epoch': 2.0}\nLoRA model saved.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport subprocess\nimport psutil\nimport pandas as pd\nimport numpy as np\nimport torch\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    TrainerCallback,\n    DataCollatorForLanguageModeling\n)\nfrom datasets import Dataset, load_from_disk\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n\n# =========================================================\n#  ÊòæÂ≠òÁõëÊéßÂáΩÊï∞\n# =========================================================\ndef print_memory_usage(tag=\"\"):\n    print(f\"\\n===== Memory Usage {tag} =====\")\n    try:\n        gpu_info = subprocess.check_output(\n            [\"nvidia-smi\", \"--query-gpu=memory.used,memory.total\",\n             \"--format=csv,noheader,nounits\"]\n        ).decode(\"utf-8\").strip().split(\"\\n\")\n\n        for i, line in enumerate(gpu_info):\n            used, total = map(int, line.split(\",\"))\n            print(f\"GPU {i}: {used}MB / {total}MB\")\n    except:\n        print(\"No GPU detected\")\n\n    ram = psutil.virtual_memory()\n    print(f\"CPU RAM: {ram.used/1024**2:.2f}MB / {ram.total/1024**2:.2f}MB\")\n    print(\"======================================\\n\")\n\n\n# =========================================================\n# CallbackÔºöÊØè 100 steps ËæìÂá∫‰∏ÄÊ¨°ÊòæÂ≠ò\n# =========================================================\nclass MemoryCallback(TrainerCallback):\n    def on_step_end(self, args, state, control, **kwargs):\n        if state.global_step % 100 == 0 and state.global_step > 0:\n            print_memory_usage(f\"Training step {state.global_step}\")\n\n\n# =========================================================\n#  Tokenized Cache ÁõÆÂΩï\n# =========================================================\ncache_dir = \"/content/drive/MyDrive/gpt2_reply_tokenized_cache\"\n\n\n# =========================================================\n# Step 1ÔºöÂä†ËΩΩÊàñÂàõÂª∫ Tokenized Cache\n# =========================================================\nif os.path.exists(cache_dir):\n    print(\"üîÑ Loading cached tokenized dataset...\")\n    tokenized_datasets = load_from_disk(cache_dir)\n\nelse:\n    print(\"‚ö† No cache found! Creating tokenized dataset...\")\n\n    # ËØªÂèñÂéüÂßãÊï∞ÊçÆ\n    print(\"Loading synthetic dataset...\")\n    df = pd.read_csv(\"/kaggle/input/generation-data/synthetic_reply_dataset.csv\")\n    dataset = Dataset.from_pandas(df)\n    dataset = dataset.train_test_split(test_size=0.1)\n\n    # Tokenizer\n    model_name = \"gpt2\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    tokenizer.pad_token = tokenizer.eos_token\n\n    # È¢ÑÂ§ÑÁêÜÂáΩÊï∞\n    def preprocess_function(examples):\n        inputs = examples[\"Message\"]\n        targets = examples[\"reply\"]\n\n        texts = []\n        for i, t in zip(inputs, targets):\n            i = str(i) if i is not None else \"\"\n            t = str(t) if t is not None else \"\"\n            text = f\"Email: {i[:512]}\\n\\nReply: {t}{tokenizer.eos_token}\"\n            texts.append(text)\n\n        return tokenizer(texts, truncation=True, padding=\"max_length\", max_length=256)\n\n    print(\"Tokenizing dataset (first time, this is slow)...\")\n    tokenized_datasets = dataset.map(preprocess_function, batched=True)\n\n    # ‰øùÂ≠òÁºìÂ≠ò\n    print(f\"Saving tokenized dataset to cache: {cache_dir}\")\n    tokenized_datasets.save_to_disk(cache_dir)\n\n    print(\"‚úÖ Cache created!\")\n\n\n# =========================================================\n# Step 2ÔºöÂä†ËΩΩÊ®°ÂûãÂíå tokenizer\n# =========================================================\nmodel_name = \"gpt2\"\nprint(f\"Loading model: {model_name}...\")\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n\n# =========================================================\n# Step 3ÔºöTrainingArgumentsÔºàÊ®°Âûã‰øùÂ≠òÂà∞ Google DriveÔºâ\n# =========================================================\noutput_dir = \"/kaggle/working/reply_generation_model_full\"\n\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=3,\n    dataloader_num_workers=0,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    save_strategy=\"epoch\",\n    eval_strategy=\"epoch\",\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    save_total_limit=2,\n    logging_dir=f\"{output_dir}/logs\",\n)\n\n\n# =========================================================\n# Step 4ÔºöÊûÑÂª∫ TrainerÔºàÂê´ÊòæÂ≠òÁõëÊéßÔºâ\n# =========================================================\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n    callbacks=[MemoryCallback()],\n)\n\n\n# =========================================================\n# Step 5ÔºöÂºÄÂßãËÆ≠ÁªÉ\n# =========================================================\nprint_memory_usage(\"Before training\")\nprint(\"üöÄ Starting training...\")\ntrainer.train()\nprint_memory_usage(\"After training\")\n\n\n# =========================================================\n# Step 6Ôºö‰øùÂ≠òÊúÄÁªàÊ®°Âûã\n# =========================================================\nprint(f\"Saving final model and tokenizer to {output_dir}...\")\ntrainer.save_model(output_dir)\ntokenizer.save_pretrained(output_dir)\n\nprint(\"‚úÖ Done! Model saved to Google Drive.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T07:53:02.590837Z","iopub.execute_input":"2025-11-23T07:53:02.591269Z","iopub.status.idle":"2025-11-23T08:24:11.659113Z","shell.execute_reply.started":"2025-11-23T07:53:02.591236Z","shell.execute_reply":"2025-11-23T08:24:11.658285Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stderr","text":"2025-11-23 07:53:15.986000: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763884396.171969      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763884396.226954      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"‚ö† No cache found! Creating tokenized dataset...\nLoading synthetic dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47d7b96f2c0a43e88c40cc180e60565d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fe03f1bde054b7385fb856f7b0a2e80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63aa0edf573344959028b35731df2679"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f73bc92165d149e484b156de244f7189"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"260afa8092414e128e0d2c0fcdab0721"}},"metadata":{}},{"name":"stdout","text":"Tokenizing dataset (first time, this is slow)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14843 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"790afbf1675d4c77a906cb313db489c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1650 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7abf5136925346a7b77abe05fb61d6c6"}},"metadata":{}},{"name":"stdout","text":"Saving tokenized dataset to cache: /content/drive/MyDrive/gpt2_reply_tokenized_cache\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/14843 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4682e9bdbcc74167bb3a7374c1854723"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/1650 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d0222f9dcc848c7ba006f859de073e5"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Cache created!\nLoading model: gpt2...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb4ec3f8f14444e29c6f500e94f635e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38b5bfd85bda4caeaa96f1bd854b1157"}},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"\n===== Memory Usage Before training =====\nGPU 0: 645MB / 15360MB\nGPU 1: 3MB / 15360MB\nCPU RAM: 2661.81MB / 32102.90MB\n======================================\n\nüöÄ Starting training...\n","output_type":"stream"},{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1392' max='1392' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1392/1392 30:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>2.375518</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.340400</td>\n      <td>2.282806</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.186000</td>\n      <td>2.259830</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\n===== Memory Usage Training step 100 =====\nGPU 0: 9797MB / 15360MB\nGPU 1: 105MB / 15360MB\nCPU RAM: 3374.21MB / 32102.90MB\n======================================\n\n\n===== Memory Usage Training step 200 =====\nGPU 0: 9797MB / 15360MB\nGPU 1: 105MB / 15360MB\nCPU RAM: 3371.68MB / 32102.90MB\n======================================\n\n\n===== Memory Usage Training step 300 =====\nGPU 0: 9797MB / 15360MB\nGPU 1: 105MB / 15360MB\nCPU RAM: 3352.39MB / 32102.90MB\n======================================\n\n\n===== Memory Usage Training step 400 =====\nGPU 0: 9797MB / 15360MB\nGPU 1: 105MB / 15360MB\nCPU RAM: 3357.43MB / 32102.90MB\n======================================\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n===== Memory Usage Training step 500 =====\nGPU 0: 11395MB / 15360MB\nGPU 1: 3149MB / 15360MB\nCPU RAM: 3557.25MB / 32102.90MB\n======================================\n\n\n===== Memory Usage Training step 600 =====\nGPU 0: 11395MB / 15360MB\nGPU 1: 3149MB / 15360MB\nCPU RAM: 3572.18MB / 32102.90MB\n======================================\n\n\n===== Memory Usage Training step 700 =====\nGPU 0: 11395MB / 15360MB\nGPU 1: 3149MB / 15360MB\nCPU RAM: 3575.00MB / 32102.90MB\n======================================\n\n\n===== Memory Usage Training step 800 =====\nGPU 0: 11395MB / 15360MB\nGPU 1: 3149MB / 15360MB\nCPU RAM: 3563.14MB / 32102.90MB\n======================================\n\n\n===== Memory Usage Training step 900 =====\nGPU 0: 11395MB / 15360MB\nGPU 1: 3149MB / 15360MB\nCPU RAM: 3554.83MB / 32102.90MB\n======================================\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n===== Memory Usage Training step 1000 =====\nGPU 0: 11395MB / 15360MB\nGPU 1: 3149MB / 15360MB\nCPU RAM: 3583.08MB / 32102.90MB\n======================================\n\n\n===== Memory Usage Training step 1100 =====\nGPU 0: 11395MB / 15360MB\nGPU 1: 3149MB / 15360MB\nCPU RAM: 3582.09MB / 32102.90MB\n======================================\n\n\n===== Memory Usage Training step 1200 =====\nGPU 0: 11395MB / 15360MB\nGPU 1: 3149MB / 15360MB\nCPU RAM: 3593.32MB / 32102.90MB\n======================================\n\n\n===== Memory Usage Training step 1300 =====\nGPU 0: 11395MB / 15360MB\nGPU 1: 3149MB / 15360MB\nCPU RAM: 3571.47MB / 32102.90MB\n======================================\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n===== Memory Usage After training =====\nGPU 0: 11395MB / 15360MB\nGPU 1: 3149MB / 15360MB\nCPU RAM: 3572.68MB / 32102.90MB\n======================================\n\nSaving final model and tokenizer to /kaggle/working/reply_generation_model_full...\n‚úÖ Done! Model saved to Google Drive.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport subprocess\nimport psutil\nimport pandas as pd\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    TrainerCallback,\n    DataCollatorForLanguageModeling\n)\nfrom datasets import Dataset, load_from_disk\nfrom peft import LoraConfig, get_peft_model\n\n# Á¶ÅÁî® W&B\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n\n# =========================================================\n# ÊòæÂ≠òÁõëÊéßÂáΩÊï∞\n# =========================================================\ndef print_memory_usage(tag=\"\"):\n    print(f\"\\n===== Memory Usage {tag} =====\")\n    try:\n        gpu_info = subprocess.check_output(\n            [\"nvidia-smi\", \"--query-gpu=memory.used,memory.total\",\n             \"--format=csv,noheader,nounits\"]\n        ).decode(\"utf-8\").strip().split(\"\\n\")\n\n        for i, line in enumerate(gpu_info):\n            used, total = map(int, line.split(\",\"))\n            print(f\"GPU {i}: {used}MB / {total}MB\")\n    except:\n        print(\"No GPU detected\")\n\n    ram = psutil.virtual_memory()\n    print(f\"CPU RAM: {ram.used/1024**2:.2f}MB / {ram.total/1024**2:.2f}MB\")\n    print(\"======================================\\n\")\n\n\n# =========================================================\n# CallbackÔºöÊØè 100 steps ËæìÂá∫ÊòæÂ≠ò\n# =========================================================\nclass MemoryCallback(TrainerCallback):\n    def on_step_end(self, args, state, control, **kwargs):\n        if state.global_step % 100 == 0 and state.global_step > 0:\n            print_memory_usage(f\"Training step {state.global_step}\")\n\n\n# =========================================================\n# Tokenized Cache ÁõÆÂΩï\n# =========================================================\ncache_dir = \"/content/drive/MyDrive/gpt2_reply_tokenized_cache_lora\"\n\n\n# =========================================================\n# Step 1ÔºöÂä†ËΩΩÊàñÂàõÂª∫ Tokenized Cache\n# =========================================================\nif os.path.exists(cache_dir):\n    print(\"üîÑ Loading cached tokenized dataset...\")\n    tokenized_datasets = load_from_disk(cache_dir)\nelse:\n    print(\"‚ö† No cache found! Creating tokenized dataset...\")\n\n    print(\"Loading synthetic dataset...\")\n    df = pd.read_csv(\"/kaggle/input/generation-data/synthetic_reply_dataset.csv\")\n    dataset = Dataset.from_pandas(df)\n    dataset = dataset.train_test_split(test_size=0.1)\n\n    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n    tokenizer.pad_token = tokenizer.eos_token\n\n    def preprocess_function(examples):\n        inputs = examples[\"Message\"]\n        targets = examples[\"reply\"]\n\n        texts = []\n        for i, t in zip(inputs, targets):\n            i = str(i) if i else \"\"\n            t = str(t) if t else \"\"\n            text = f\"Email: {i[:512]}\\n\\nReply: {t}{tokenizer.eos_token}\"\n            texts.append(text)\n\n        return tokenizer(texts, truncation=True, padding=\"max_length\", max_length=256)\n\n    print(\"Tokenizing dataset (this may take time)...\")\n    tokenized_datasets = dataset.map(preprocess_function, batched=True)\n\n    print(f\"Saving tokenized dataset to cache: {cache_dir}\")\n    tokenized_datasets.save_to_disk(cache_dir)\n    print(\"‚úÖ Cache created!\")\n\n\n# =========================================================\n# Step 2ÔºöÂä†ËΩΩ GPT-2 + LoRA\n# =========================================================\nprint(\"Loading GPT-2 + LoRA...\")\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"gpt2\",\n    torch_dtype=torch.float16\n)\n\n# GPT-2 attention Ê®°ÂùóÂêçÁß∞\ntarget_modules = [\"c_attn\", \"c_proj\"]\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.1,\n    target_modules=target_modules,\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(base_model, lora_config)\nmodel.print_trainable_parameters()\n\n\n# =========================================================\n# Step 3ÔºöTrainingArgumentsÔºà‰∏éÂÖ®ÂèÇÊï∞ËÆ≠ÁªÉ‰∏ÄËá¥Ôºâ\n# =========================================================\noutput_dir = \"/kaggle/working/reply_generation_model_LoRA\"\n\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=3,\n    dataloader_num_workers=0,                 \n    per_device_train_batch_size=16,           \n    per_device_eval_batch_size=16,            \n    save_strategy=\"epoch\",\n    eval_strategy=\"epoch\",\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    save_total_limit=2,\n    logging_dir=f\"{output_dir}/logs\",\n    fp16=True,\n)\n\n\n# =========================================================\n# Step 4ÔºöTrainerÔºàÂê´ÊòæÂ≠òÁõëÊéßÔºâ\n# =========================================================\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n    callbacks=[MemoryCallback()],\n)\n\n\n# =========================================================\n# Step 5ÔºöËÆ≠ÁªÉ\n# =========================================================\nprint_memory_usage(\"Before LoRA Training\")\nprint(\"üöÄ Starting LoRA training...\")\ntrainer.train()\nprint_memory_usage(\"After LoRA Training\")\n\n\n# =========================================================\n# Step 6Ôºö‰øùÂ≠ò LoRA Ê®°Âûã\n# =========================================================\nprint(f\"Saving LoRA model to {output_dir}...\")\ntrainer.save_model(output_dir)\ntokenizer.save_pretrained(output_dir)\n\nprint(\"‚úÖ Done! LoRA model saved to Google Drive.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T08:29:40.688586Z","iopub.execute_input":"2025-11-23T08:29:40.689321Z","iopub.status.idle":"2025-11-23T08:39:27.030445Z","shell.execute_reply.started":"2025-11-23T08:29:40.689288Z","shell.execute_reply":"2025-11-23T08:39:27.029636Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"üîÑ Loading cached tokenized dataset...\nLoading GPT-2 + LoRA...\n","output_type":"stream"},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 811,008 || all params: 125,250,816 || trainable%: 0.6475\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"\n===== Memory Usage Before LoRA Training =====\nGPU 0: 373MB / 15360MB\nGPU 1: 3MB / 15360MB\nCPU RAM: 2947.89MB / 32102.90MB\n======================================\n\nüöÄ Starting LoRA training...\n","output_type":"stream"},{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1392' max='1392' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1392/1392 09:41, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>3.334808</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.956900</td>\n      <td>3.187673</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.714800</td>\n      <td>3.151971</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\n===== Memory Usage Training step 100 =====\nGPU 0: 6561MB / 15360MB\nGPU 1: 105MB / 15360MB\nCPU RAM: 3494.79MB / 32102.90MB\n======================================\n\n\n===== Memory Usage Training step 200 =====\nGPU 0: 6561MB / 15360MB\nGPU 1: 105MB / 15360MB\nCPU RAM: 3488.26MB / 32102.90MB\n======================================\n\n\n===== Memory Usage Training step 300 =====\nGPU 0: 6561MB / 15360MB\nGPU 1: 105MB / 15360MB\nCPU RAM: 3479.69MB / 32102.90MB\n======================================\n\n\n===== Memory Usage Training step 400 =====\nGPU 0: 6561MB / 15360MB\nGPU 1: 105MB / 15360MB\nCPU RAM: 3485.19MB / 32102.90MB\n======================================\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n===== Memory Usage Training step 500 =====\nGPU 0: 6587MB / 15360MB\nGPU 1: 2821MB / 15360MB\nCPU RAM: 3710.10MB / 32102.90MB\n======================================\n\n\n===== Memory Usage Training step 600 =====\nGPU 0: 6587MB / 15360MB\nGPU 1: 2821MB / 15360MB\nCPU RAM: 3723.44MB / 32102.90MB\n======================================\n\n\n===== Memory Usage Training step 700 =====\nGPU 0: 6587MB / 15360MB\nGPU 1: 2821MB / 15360MB\nCPU RAM: 3720.54MB / 32102.90MB\n======================================\n\n\n===== Memory Usage Training step 800 =====\nGPU 0: 6587MB / 15360MB\nGPU 1: 2821MB / 15360MB\nCPU RAM: 3704.08MB / 32102.90MB\n======================================\n\n\n===== Memory Usage Training step 900 =====\nGPU 0: 6587MB / 15360MB\nGPU 1: 2821MB / 15360MB\nCPU RAM: 3724.56MB / 32102.90MB\n======================================\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n===== Memory Usage Training step 1000 =====\nGPU 0: 6587MB / 15360MB\nGPU 1: 2821MB / 15360MB\nCPU RAM: 3718.64MB / 32102.90MB\n======================================\n\n\n===== Memory Usage Training step 1100 =====\nGPU 0: 6587MB / 15360MB\nGPU 1: 2821MB / 15360MB\nCPU RAM: 3703.11MB / 32102.90MB\n======================================\n\n\n===== Memory Usage Training step 1200 =====\nGPU 0: 6587MB / 15360MB\nGPU 1: 2821MB / 15360MB\nCPU RAM: 3700.98MB / 32102.90MB\n======================================\n\n\n===== Memory Usage Training step 1300 =====\nGPU 0: 6587MB / 15360MB\nGPU 1: 2821MB / 15360MB\nCPU RAM: 3699.83MB / 32102.90MB\n======================================\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n===== Memory Usage After LoRA Training =====\nGPU 0: 6587MB / 15360MB\nGPU 1: 2821MB / 15360MB\nCPU RAM: 3700.64MB / 32102.90MB\n======================================\n\nSaving LoRA model to /kaggle/working/reply_generation_model_LoRA...\n‚úÖ Done! LoRA model saved to Google Drive.\n","output_type":"stream"}],"execution_count":7}]}