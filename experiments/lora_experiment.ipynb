{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-23T08:29:29.110169Z",
     "iopub.status.busy": "2025-11-23T08:29:29.109563Z",
     "iopub.status.idle": "2025-11-23T08:29:29.374058Z",
     "shell.execute_reply": "2025-11-23T08:29:29.373290Z",
     "shell.execute_reply.started": "2025-11-23T08:29:29.110144Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 23 08:29:29 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   35C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   33C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-11-23T08:43:25.504810Z",
     "iopub.status.busy": "2025-11-23T08:43:25.504538Z",
     "iopub.status.idle": "2025-11-23T08:43:35.895533Z",
     "shell.execute_reply": "2025-11-23T08:43:35.894413Z",
     "shell.execute_reply.started": "2025-11-23T08:43:25.504790Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.18)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
      "Collecting pyarrow>=21.0.0 (from datasets>=2.0.0->evaluate)\n",
      "  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (0.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.10.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow, evaluate\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 19.0.1\n",
      "    Uninstalling pyarrow-19.0.1:\n",
      "      Successfully uninstalled pyarrow-19.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "pylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "cudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "cudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed evaluate-0.4.6 pyarrow-22.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T06:19:59.039618Z",
     "iopub.status.busy": "2025-11-23T06:19:59.039342Z",
     "iopub.status.idle": "2025-11-23T06:19:59.045118Z",
     "shell.execute_reply": "2025-11-23T06:19:59.044383Z",
     "shell.execute_reply.started": "2025-11-23T06:19:59.039597Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input directory: ['.virtual_documents']\n",
      "/kaggle/working ['.virtual_documents'] []\n",
      "/kaggle/working/.virtual_documents [] []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Input directory:\", os.listdir('/kaggle/input'))\n",
    "\n",
    "for root, dirs, files in os.walk('/kaggle/input', topdown=True):\n",
    "    print(root, dirs, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T08:43:38.204643Z",
     "iopub.status.busy": "2025-11-23T08:43:38.204131Z",
     "iopub.status.idle": "2025-11-23T09:09:53.735472Z",
     "shell.execute_reply": "2025-11-23T09:09:53.734814Z",
     "shell.execute_reply.started": "2025-11-23T08:43:38.204611Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 08:43:59.951971: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763887440.336696      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763887440.478062      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üÜï No cache found ‚Äî processing raw dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6bbc3fdb424f468e0abbe001679410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "310d062216294d8b98d20e45a8666f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52cc182499614df2911c602d55a4d04f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07fd9c470ed24713ad4ef175c58727bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11df9b559f34da3b70a67ec52d36d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26972 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736eadbaf7c349c283b957dc25bd8482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6744 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12534b16683c487c888109662e0ab0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/26972 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dca2b41436d4487ae1a4f58b932ccd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6744 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a1e59c01604f0189a6305907a038a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26671 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d3f599490e4ac1b25a7899c7e3da1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6674 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving tokenized dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6510f1c3a254e178843bc09ec5f2347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/26671 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3836e7553664b41a88563272003b1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6674 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úî Tokenized dataset saved to /tokenized_enron_spam\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2dbd2f093b4b5987b9654140ce9f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3db9f45a15a4e5e9ab125b5fecca622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipykernel_48/1982602989.py:137: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Memory Usage Before training =====\n",
      "GPU 0: 395MB / 15360MB\n",
      "GPU 1: 3MB / 15360MB\n",
      "CPU RAM: 2728.25MB / 32102.90MB\n",
      "====================================\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='418' max='418' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [418/418 23:48, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.034197</td>\n",
       "      <td>0.987863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.030358</td>\n",
       "      <td>0.990261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Memory Usage Training step 100 =====\n",
      "GPU 0: 11411MB / 15360MB\n",
      "GPU 1: 10621MB / 15360MB\n",
      "CPU RAM: 3498.99MB / 32102.90MB\n",
      "====================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Memory Usage Training step 200 =====\n",
      "GPU 0: 11411MB / 15360MB\n",
      "GPU 1: 10621MB / 15360MB\n",
      "CPU RAM: 3475.33MB / 32102.90MB\n",
      "====================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Memory Usage Training step 300 =====\n",
      "GPU 0: 11411MB / 15360MB\n",
      "GPU 1: 10621MB / 15360MB\n",
      "CPU RAM: 3513.51MB / 32102.90MB\n",
      "====================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Memory Usage Training step 400 =====\n",
      "GPU 0: 11411MB / 15360MB\n",
      "GPU 1: 10621MB / 15360MB\n",
      "CPU RAM: 3506.30MB / 32102.90MB\n",
      "====================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Memory Usage After training =====\n",
      "GPU 0: 11411MB / 15360MB\n",
      "GPU 1: 10621MB / 15360MB\n",
      "CPU RAM: 3494.85MB / 32102.90MB\n",
      "====================================\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 00:56]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.030358023941516876, 'eval_accuracy': 0.9902607132154629, 'eval_runtime': 58.0805, 'eval_samples_per_second': 114.91, 'eval_steps_per_second': 0.913, 'epoch': 2.0}\n",
      "Model saved to spam_classifier_model__full\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import TrainerCallback\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import subprocess\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "\n",
    "# ====================== Function to check the memory usage\n",
    "def print_memory_usage(tag=\"\"):\n",
    "    print(f\"\\n===== Memory Usage {tag} =====\")\n",
    "    try:\n",
    "        gpu_info = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.used,memory.total\", \"--format=csv,noheader,nounits\"]\n",
    "        )\n",
    "        gpu_info = gpu_info.decode(\"utf-8\").strip().split(\"\\n\")\n",
    "        for i, gpu in enumerate(gpu_info):\n",
    "            used, total = map(int, gpu.split(','))\n",
    "            print(f\"GPU {i}: {used}MB / {total}MB\")\n",
    "    except:\n",
    "        print(\"No GPU found\")\n",
    "\n",
    "    ram = psutil.virtual_memory()\n",
    "    print(f\"CPU RAM: {ram.used/1024**2:.2f}MB / {ram.total/1024**2:.2f}MB\")\n",
    "    print(\"====================================\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MemoryCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 100 == 0 and state.global_step > 0:\n",
    "            print_memory_usage(f\"Training step {state.global_step}\")\n",
    "\n",
    "\n",
    "cache_path = \"/tokenized_enron_spam\"\n",
    "\n",
    "if os.path.exists(cache_path):\n",
    "    tokenized_datasets = load_from_disk(cache_path)\n",
    "\n",
    "else:\n",
    "    df = pd.read_csv(\"/kaggle/input/classification-data/enron_spam_data.csv\")\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "    model_name = \"distilbert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def prepare_dataset(example):\n",
    "        example[\"label\"] = 1 if example[\"Spam/Ham\"].lower() == \"spam\" else 0\n",
    "        return example\n",
    "\n",
    "    dataset = dataset.map(prepare_dataset)\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        messages = [msg if msg is not None else \"\" for msg in examples[\"Message\"]]\n",
    "        return tokenizer(messages, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    dataset = dataset.filter(lambda x: x[\"Message\"] is not None and len(str(x[\"Message\"]).strip()) > 0)\n",
    "\n",
    "    tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "    print(\"üíæ Saving tokenized dataset...\")\n",
    "    tokenized_datasets.save_to_disk(cache_path)\n",
    "    print(f\"‚úî Tokenized dataset saved to {cache_path}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Label map\n",
    "id2label = {0: \"HAM\", 1: \"SPAM\"}\n",
    "label2id = {\"HAM\": 0, \"SPAM\": 1}\n",
    "\n",
    "# ====================== Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"kagglespam_classification_results\",\n",
    "    learning_rate=2e-5,\n",
    "    dataloader_num_workers=0,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[MemoryCallback()],\n",
    ")\n",
    "\n",
    "print_memory_usage(\"Before training\")\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print_memory_usage(\"After training\")\n",
    "\n",
    "print(\"Evaluating...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")\n",
    "\n",
    "trainer.save_model(\"spam_classifier_model__full\")\n",
    "print(\"Model saved to spam_classifier_model__full\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-11-23T06:07:33.829653Z",
     "iopub.status.busy": "2025-11-23T06:07:33.829013Z",
     "iopub.status.idle": "2025-11-23T06:15:48.423100Z",
     "shell.execute_reply": "2025-11-23T06:15:48.422220Z",
     "shell.execute_reply.started": "2025-11-23T06:07:33.829621Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 06:07:55.601300: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763878076.023848      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763878076.145067      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üÜï No cache found ‚Äî processing raw dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a59652c10074c1a903dbd7bff600902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32a078af9bf4de187c80aa01070fada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6dd6a2cbd44b5f95d98507601baf7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d54ed13f914964a5bad1864b6d85f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c052a3dcc724346828619e1fbcf3883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26972 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543e7b3c2f0c46849ebeb2ff5cb3b135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6744 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7939a436a94ec59e9ce34582a37a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/26972 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a11dc10f064b7a878ac6828b8374b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6744 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8127ac65dca448c3bba069777643b0df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26684 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d397b3e4084290b3c7af6382245278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6661 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving tokenized dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0323c3fb5c4bfe9c18c14504a83d73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/26684 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fec45b8c7384ea0ba48ff301875ab88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6661 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úî Tokenized dataset saved to /tokenized_enron_spam\n",
      "Loading DistilBERT for LoRA...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346699eeadc0469c96f3fc4171128bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,181,954 || all params: 68,136,964 || trainable%: 1.7347\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d05d4725dad422ba679327188372c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48/2105563729.py:164: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Memory Usage Before LoRA training =====\n",
      "GPU 0: 253MB / 15360MB\n",
      "GPU 1: 3MB / 15360MB\n",
      "CPU RAM: 2687.07MB / 32102.89MB\n",
      "====================================\n",
      "\n",
      "üöÄ Starting LoRA training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='418' max='418' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [418/418 06:08, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.487614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.487614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Memory Usage Training step 100 =====\n",
      "GPU 0: 6991MB / 15360MB\n",
      "GPU 1: 6989MB / 15360MB\n",
      "CPU RAM: 3518.64MB / 32102.89MB\n",
      "====================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Memory Usage Training step 200 =====\n",
      "GPU 0: 6991MB / 15360MB\n",
      "GPU 1: 6989MB / 15360MB\n",
      "CPU RAM: 3517.01MB / 32102.89MB\n",
      "====================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:3177: RuntimeWarning: invalid value encountered in less\n",
      "  if operator(metric_value, self.state.best_metric):\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Memory Usage Training step 300 =====\n",
      "GPU 0: 7017MB / 15360MB\n",
      "GPU 1: 7015MB / 15360MB\n",
      "CPU RAM: 3506.80MB / 32102.89MB\n",
      "====================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Memory Usage Training step 400 =====\n",
      "GPU 0: 7017MB / 15360MB\n",
      "GPU 1: 7015MB / 15360MB\n",
      "CPU RAM: 3511.20MB / 32102.89MB\n",
      "====================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:3177: RuntimeWarning: invalid value encountered in less\n",
      "  if operator(metric_value, self.state.best_metric):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Memory Usage After LoRA training =====\n",
      "GPU 0: 7017MB / 15360MB\n",
      "GPU 1: 7015MB / 15360MB\n",
      "CPU RAM: 3501.22MB / 32102.89MB\n",
      "====================================\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': nan, 'eval_accuracy': 0.4876144723014562, 'eval_runtime': 18.5303, 'eval_samples_per_second': 359.464, 'eval_steps_per_second': 2.86, 'epoch': 2.0}\n",
      "LoRA model saved.\n"
     ]
    }
   ],
   "source": [
    "# We also have to import those librarys again in each new cell, as the code running on kaggle would not release the memory automatically, we have to reset the environment each time\n",
    "import os\n",
    "import subprocess\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TrainerCallback\n",
    ")\n",
    "from datasets import Dataset, load_from_disk\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import evaluate  \n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Same , we need to reset the environment, function in a new cell\n",
    "def print_memory_usage(tag=\"\"):\n",
    "    print(f\"\\n===== Memory Usage {tag} =====\")\n",
    "    try:\n",
    "        gpu_info = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.used,memory.total\", \"--format=csv,noheader,nounits\"]\n",
    "        )\n",
    "        gpu_info = gpu_info.decode(\"utf-8\").strip().split(\"\\n\")\n",
    "        for i, gpu in enumerate(gpu_info):\n",
    "            used, total = map(int, gpu.split(','))\n",
    "            print(f\"GPU {i}: {used}MB / {total}MB\")\n",
    "    except Exception as e:\n",
    "        print(\"No GPU found or nvidia-smi not available\")\n",
    "\n",
    "    ram = psutil.virtual_memory()\n",
    "    print(f\"CPU RAM: {ram.used/1024**2:.2f}MB / {ram.total/1024**2:.2f}MB\")\n",
    "    print(\"====================================\\n\")\n",
    "\n",
    "\n",
    "class MemoryCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 100 == 0 and state.global_step > 0:\n",
    "            print_memory_usage(f\"Training step {state.global_step}\")\n",
    "\n",
    "cache_path = \"/tokenized_enron_spam\"\n",
    "\n",
    "if os.path.exists(cache_path):\n",
    "    tokenized_datasets = load_from_disk(cache_path)\n",
    "else:\n",
    "    df = pd.read_csv(\"/kaggle/input/classification-data/enron_spam_data.csv\")\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "    model_name = \"distilbert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def prepare_dataset(example):\n",
    "        example[\"label\"] = 1 if example[\"Spam/Ham\"].lower() == \"spam\" else 0\n",
    "        return example\n",
    "\n",
    "    dataset = dataset.map(prepare_dataset)\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        messages = [msg if msg is not None else \"\" for msg in examples[\"Message\"]]\n",
    "        return tokenizer(messages, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    dataset = dataset.filter(lambda x: x[\"Message\"] is not None and len(str(x[\"Message\"]).strip()) > 0)\n",
    "\n",
    "    tokenized_datasets = dataset.map(preprocess_function, batched=True, batch_size=1000)\n",
    "\n",
    "    print(\"Saving tokenized dataset...\")\n",
    "    tokenized_datasets.save_to_disk(cache_path)\n",
    "    print(f\"‚úî Tokenized dataset saved to {cache_path}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "id2label = {0: \"HAM\", 1: \"SPAM\"}\n",
    "label2id = {\"HAM\": 0, \"SPAM\": 1}\n",
    "\n",
    "\n",
    "# =================================== Load Model with LoRA\n",
    "print(\"Loading DistilBERT for LoRA...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    torch_dtype=torch.float16,  # FP16 for memory efficiency\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"],  # DistilBERT specific\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/kaggle/working/spam_classification_LoRA_result\",\n",
    "    learning_rate=1e-4,\n",
    "    dataloader_num_workers=0,  # Avoid tokenizers fork warning\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    max_grad_norm=1.0,\n",
    "    report_to=\"none\",  \n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[MemoryCallback()],\n",
    ")\n",
    "\n",
    "print_memory_usage(\"Before LoRA training\")\n",
    "print(\"üöÄ Starting LoRA training...\")\n",
    "trainer.train()\n",
    "print_memory_usage(\"After LoRA training\")\n",
    "\n",
    "print(\"Evaluating...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")\n",
    "\n",
    "trainer.save_model(\"/kaggle/working/spam_classifier_LoRA_model\")\n",
    "print(\"LoRA model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-11-23T07:53:02.591269Z",
     "iopub.status.busy": "2025-11-23T07:53:02.590837Z",
     "iopub.status.idle": "2025-11-23T08:24:11.659113Z",
     "shell.execute_reply": "2025-11-23T08:24:11.658285Z",
     "shell.execute_reply.started": "2025-11-23T07:53:02.591236Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 07:53:15.986000: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763884396.171969      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763884396.226954      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö† No cache found! Creating tokenized dataset...\n",
      "Loading synthetic dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d7b96f2c0a43e88c40cc180e60565d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe03f1bde054b7385fb856f7b0a2e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63aa0edf573344959028b35731df2679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f73bc92165d149e484b156de244f7189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260afa8092414e128e0d2c0fcdab0721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset (first time, this is slow)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790afbf1675d4c77a906cb313db489c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14843 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7abf5136925346a7b77abe05fb61d6c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1650 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving tokenized dataset to cache: /content/drive/MyDrive/gpt2_reply_tokenized_cache\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4682e9bdbcc74167bb3a7374c1854723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/14843 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0222f9dcc848c7ba006f859de073e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1650 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cache created!\n",
      "Loading model: gpt2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb4ec3f8f14444e29c6f500e94f635e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b5bfd85bda4caeaa96f1bd854b1157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Memory Usage Before training =====\n",
      "GPU 0: 645MB / 15360MB\n",
      "GPU 1: 3MB / 15360MB\n",
      "CPU RAM: 2661.81MB / 32102.90MB\n",
      "======================================\n",
      "\n",
      "üöÄ Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1392' max='1392' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1392/1392 30:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.375518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.340400</td>\n",
       "      <td>2.282806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.186000</td>\n",
       "      <td>2.259830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Memory Usage Training step 100 =====\n",
      "GPU 0: 9797MB / 15360MB\n",
      "GPU 1: 105MB / 15360MB\n",
      "CPU RAM: 3374.21MB / 32102.90MB\n",
      "======================================\n",
      "\n",
      "\n",
      "===== Memory Usage Training step 200 =====\n",
      "GPU 0: 9797MB / 15360MB\n",
      "GPU 1: 105MB / 15360MB\n",
      "CPU RAM: 3371.68MB / 32102.90MB\n",
      "======================================\n",
      "\n",
      "\n",
      "===== Memory Usage Training step 300 =====\n",
      "GPU 0: 9797MB / 15360MB\n",
      "GPU 1: 105MB / 15360MB\n",
      "CPU RAM: 3352.39MB / 32102.90MB\n",
      "======================================\n",
      "\n",
      "\n",
      "===== Memory Usage Training step 400 =====\n",
      "GPU 0: 9797MB / 15360MB\n",
      "GPU 1: 105MB / 15360MB\n",
      "CPU RAM: 3357.43MB / 32102.90MB\n",
      "======================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Memory Usage Training step 500 =====\n",
      "GPU 0: 11395MB / 15360MB\n",
      "GPU 1: 3149MB / 15360MB\n",
      "CPU RAM: 3557.25MB / 32102.90MB\n",
      "======================================\n",
      "\n",
      "\n",
      "===== Memory Usage Training step 600 =====\n",
      "GPU 0: 11395MB / 15360MB\n",
      "GPU 1: 3149MB / 15360MB\n",
      "CPU RAM: 3572.18MB / 32102.90MB\n",
      "======================================\n",
      "\n",
      "\n",
      "===== Memory Usage Training step 700 =====\n",
      "GPU 0: 11395MB / 15360MB\n",
      "GPU 1: 3149MB / 15360MB\n",
      "CPU RAM: 3575.00MB / 32102.90MB\n",
      "======================================\n",
      "\n",
      "\n",
      "===== Memory Usage Training step 800 =====\n",
      "GPU 0: 11395MB / 15360MB\n",
      "GPU 1: 3149MB / 15360MB\n",
      "CPU RAM: 3563.14MB / 32102.90MB\n",
      "======================================\n",
      "\n",
      "\n",
      "===== Memory Usage Training step 900 =====\n",
      "GPU 0: 11395MB / 15360MB\n",
      "GPU 1: 3149MB / 15360MB\n",
      "CPU RAM: 3554.83MB / 32102.90MB\n",
      "======================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Memory Usage Training step 1000 =====\n",
      "GPU 0: 11395MB / 15360MB\n",
      "GPU 1: 3149MB / 15360MB\n",
      "CPU RAM: 3583.08MB / 32102.90MB\n",
      "======================================\n",
      "\n",
      "\n",
      "===== Memory Usage Training step 1100 =====\n",
      "GPU 0: 11395MB / 15360MB\n",
      "GPU 1: 3149MB / 15360MB\n",
      "CPU RAM: 3582.09MB / 32102.90MB\n",
      "======================================\n",
      "\n",
      "\n",
      "===== Memory Usage Training step 1200 =====\n",
      "GPU 0: 11395MB / 15360MB\n",
      "GPU 1: 3149MB / 15360MB\n",
      "CPU RAM: 3593.32MB / 32102.90MB\n",
      "======================================\n",
      "\n",
      "\n",
      "===== Memory Usage Training step 1300 =====\n",
      "GPU 0: 11395MB / 15360MB\n",
      "GPU 1: 3149MB / 15360MB\n",
      "CPU RAM: 3571.47MB / 32102.90MB\n",
      "======================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Memory Usage After training =====\n",
      "GPU 0: 11395MB / 15360MB\n",
      "GPU 1: 3149MB / 15360MB\n",
      "CPU RAM: 3572.68MB / 32102.90MB\n",
      "======================================\n",
      "\n",
      "Saving final model and tokenizer to /kaggle/working/reply_generation_model_full...\n",
      "‚úÖ Done! Model saved to Google Drive.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "\n",
    "def print_memory_usage(tag=\"\"):\n",
    "    print(f\"\\n===== Memory Usage {tag} =====\")\n",
    "    try:\n",
    "        gpu_info = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.used,memory.total\",\n",
    "             \"--format=csv,noheader,nounits\"]\n",
    "        ).decode(\"utf-8\").strip().split(\"\\n\")\n",
    "\n",
    "        for i, line in enumerate(gpu_info):\n",
    "            used, total = map(int, line.split(\",\"))\n",
    "            print(f\"GPU {i}: {used}MB / {total}MB\")\n",
    "    except:\n",
    "        print(\"No GPU detected\")\n",
    "\n",
    "    ram = psutil.virtual_memory()\n",
    "    print(f\"CPU RAM: {ram.used/1024**2:.2f}MB / {ram.total/1024**2:.2f}MB\")\n",
    "    print(\"======================================\\n\")\n",
    "\n",
    "class MemoryCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 100 == 0 and state.global_step > 0:\n",
    "            print_memory_usage(f\"Training step {state.global_step}\")\n",
    "\n",
    "cache_dir = \"/content/drive/MyDrive/gpt2_reply_tokenized_cache\"\n",
    "\n",
    "\n",
    "if os.path.exists(cache_dir):\n",
    "    tokenized_datasets = load_from_disk(cache_dir)\n",
    "\n",
    "else:\n",
    "    df = pd.read_csv(\"/kaggle/input/generation-data/synthetic_reply_dataset.csv\")\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "    model_name = \"gpt2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        inputs = examples[\"Message\"]\n",
    "        targets = examples[\"reply\"]\n",
    "\n",
    "        texts = []\n",
    "        for i, t in zip(inputs, targets):\n",
    "            i = str(i) if i is not None else \"\"\n",
    "            t = str(t) if t is not None else \"\"\n",
    "            text = f\"Email: {i[:512]}\\n\\nReply: {t}{tokenizer.eos_token}\"\n",
    "            texts.append(text)\n",
    "\n",
    "        return tokenizer(texts, truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "    print(\"Tokenizing dataset (first time, this is slow)...\")\n",
    "    tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "    print(f\"Saving tokenized dataset to cache: {cache_dir}\")\n",
    "    tokenized_datasets.save_to_disk(cache_dir)\n",
    "\n",
    "    print(\"‚úÖ Cache created!\")\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "print(f\"Loading model: {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "output_dir = \"/kaggle/working/reply_generation_model_full\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,\n",
    "    dataloader_num_workers=0,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    callbacks=[MemoryCallback()],\n",
    ")\n",
    "\n",
    "print_memory_usage(\"Before training\")\n",
    "print(\"üöÄ Starting training...\")\n",
    "trainer.train()\n",
    "print_memory_usage(\"After training\")\n",
    "\n",
    "print(f\"Saving final model and tokenizer to {output_dir}...\")\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\" Model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-11-23T08:29:40.689321Z",
     "iopub.status.busy": "2025-11-23T08:29:40.688586Z",
     "iopub.status.idle": "2025-11-23T08:39:27.030445Z",
     "shell.execute_reply": "2025-11-23T08:39:27.029636Z",
     "shell.execute_reply.started": "2025-11-23T08:29:40.689288Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading cached tokenized dataset...\n",
      "Loading GPT-2 + LoRA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 811,008 || all params: 125,250,816 || trainable%: 0.6475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Memory Usage Before LoRA Training =====\n",
      "GPU 0: 373MB / 15360MB\n",
      "GPU 1: 3MB / 15360MB\n",
      "CPU RAM: 2947.89MB / 32102.90MB\n",
      "======================================\n",
      "\n",
      "üöÄ Starting LoRA training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1392' max='1392' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1392/1392 09:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.334808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.956900</td>\n",
       "      <td>3.187673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.714800</td>\n",
       "      <td>3.151971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Memory Usage Training step 100 =====\n",
      "GPU 0: 6561MB / 15360MB\n",
      "GPU 1: 105MB / 15360MB\n",
      "CPU RAM: 3494.79MB / 32102.90MB\n",
      "======================================\n",
      "\n",
      "\n",
      "===== Memory Usage Training step 200 =====\n",
      "GPU 0: 6561MB / 15360MB\n",
      "GPU 1: 105MB / 15360MB\n",
      "CPU RAM: 3488.26MB / 32102.90MB\n",
      "======================================\n",
      "\n",
      "\n",
      "===== Memory Usage Training step 300 =====\n",
      "GPU 0: 6561MB / 15360MB\n",
      "GPU 1: 105MB / 15360MB\n",
      "CPU RAM: 3479.69MB / 32102.90MB\n",
      "======================================\n",
      "\n",
      "\n",
      "===== Memory Usage Training step 400 =====\n",
      "GPU 0: 6561MB / 15360MB\n",
      "GPU 1: 105MB / 15360MB\n",
      "CPU RAM: 3485.19MB / 32102.90MB\n",
      "======================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Memory Usage Training step 500 =====\n",
      "GPU 0: 6587MB / 15360MB\n",
      "GPU 1: 2821MB / 15360MB\n",
      "CPU RAM: 3710.10MB / 32102.90MB\n",
      "======================================\n",
      "\n",
      "\n",
      "===== Memory Usage Training step 600 =====\n",
      "GPU 0: 6587MB / 15360MB\n",
      "GPU 1: 2821MB / 15360MB\n",
      "CPU RAM: 3723.44MB / 32102.90MB\n",
      "======================================\n",
      "\n",
      "\n",
      "===== Memory Usage Training step 700 =====\n",
      "GPU 0: 6587MB / 15360MB\n",
      "GPU 1: 2821MB / 15360MB\n",
      "CPU RAM: 3720.54MB / 32102.90MB\n",
      "======================================\n",
      "\n",
      "\n",
      "===== Memory Usage Training step 800 =====\n",
      "GPU 0: 6587MB / 15360MB\n",
      "GPU 1: 2821MB / 15360MB\n",
      "CPU RAM: 3704.08MB / 32102.90MB\n",
      "======================================\n",
      "\n",
      "\n",
      "===== Memory Usage Training step 900 =====\n",
      "GPU 0: 6587MB / 15360MB\n",
      "GPU 1: 2821MB / 15360MB\n",
      "CPU RAM: 3724.56MB / 32102.90MB\n",
      "======================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Memory Usage Training step 1000 =====\n",
      "GPU 0: 6587MB / 15360MB\n",
      "GPU 1: 2821MB / 15360MB\n",
      "CPU RAM: 3718.64MB / 32102.90MB\n",
      "======================================\n",
      "\n",
      "\n",
      "===== Memory Usage Training step 1100 =====\n",
      "GPU 0: 6587MB / 15360MB\n",
      "GPU 1: 2821MB / 15360MB\n",
      "CPU RAM: 3703.11MB / 32102.90MB\n",
      "======================================\n",
      "\n",
      "\n",
      "===== Memory Usage Training step 1200 =====\n",
      "GPU 0: 6587MB / 15360MB\n",
      "GPU 1: 2821MB / 15360MB\n",
      "CPU RAM: 3700.98MB / 32102.90MB\n",
      "======================================\n",
      "\n",
      "\n",
      "===== Memory Usage Training step 1300 =====\n",
      "GPU 0: 6587MB / 15360MB\n",
      "GPU 1: 2821MB / 15360MB\n",
      "CPU RAM: 3699.83MB / 32102.90MB\n",
      "======================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Memory Usage After LoRA Training =====\n",
      "GPU 0: 6587MB / 15360MB\n",
      "GPU 1: 2821MB / 15360MB\n",
      "CPU RAM: 3700.64MB / 32102.90MB\n",
      "======================================\n",
      "\n",
      "Saving LoRA model to /kaggle/working/reply_generation_model_LoRA...\n",
      "‚úÖ Done! LoRA model saved to Google Drive.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset, load_from_disk\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "def print_memory_usage(tag=\"\"):\n",
    "    print(f\"\\n===== Memory Usage {tag} =====\")\n",
    "    try:\n",
    "        gpu_info = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.used,memory.total\",\n",
    "             \"--format=csv,noheader,nounits\"]\n",
    "        ).decode(\"utf-8\").strip().split(\"\\n\")\n",
    "\n",
    "        for i, line in enumerate(gpu_info):\n",
    "            used, total = map(int, line.split(\",\"))\n",
    "            print(f\"GPU {i}: {used}MB / {total}MB\")\n",
    "    except:\n",
    "        print(\"No GPU detected\")\n",
    "\n",
    "    ram = psutil.virtual_memory()\n",
    "    print(f\"CPU RAM: {ram.used/1024**2:.2f}MB / {ram.total/1024**2:.2f}MB\")\n",
    "    print(\"======================================\\n\")\n",
    "\n",
    "class MemoryCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 100 == 0 and state.global_step > 0:\n",
    "            print_memory_usage(f\"Training step {state.global_step}\")\n",
    "\n",
    "cache_dir = \"/content/drive/MyDrive/gpt2_reply_tokenized_cache_lora\"\n",
    "\n",
    "if os.path.exists(cache_dir):\n",
    "    tokenized_datasets = load_from_disk(cache_dir)\n",
    "else:\n",
    "    print(\"Loading synthetic dataset...\")\n",
    "    df = pd.read_csv(\"/kaggle/input/generation-data/synthetic_reply_dataset.csv\")\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        inputs = examples[\"Message\"]\n",
    "        targets = examples[\"reply\"]\n",
    "\n",
    "        texts = []\n",
    "        for i, t in zip(inputs, targets):\n",
    "            i = str(i) if i else \"\"\n",
    "            t = str(t) if t else \"\"\n",
    "            text = f\"Email: {i[:512]}\\n\\nReply: {t}{tokenizer.eos_token}\"\n",
    "            texts.append(text)\n",
    "\n",
    "        return tokenizer(texts, truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "    print(\"Tokenizing dataset (this may take time)...\")\n",
    "    tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "    print(f\"Saving tokenized dataset to cache: {cache_dir}\")\n",
    "    tokenized_datasets.save_to_disk(cache_dir)\n",
    "    print(\"Cache created!\")\n",
    "\n",
    "# Loding GPT-2 with LoRA\n",
    "print(\"Loading GPT-2 + LoRA...\")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "target_modules = [\"c_attn\", \"c_proj\"]\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=target_modules,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "output_dir = \"/kaggle/working/reply_generation_model_LoRA\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,\n",
    "    dataloader_num_workers=0,                 \n",
    "    per_device_train_batch_size=16,           \n",
    "    per_device_eval_batch_size=16,            \n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-4, # 5e-5 -> 2e-4\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    callbacks=[MemoryCallback()],\n",
    ")\n",
    "\n",
    "print_memory_usage(\"Before LoRA Training\")\n",
    "print(\"Starting LoRA training...\")\n",
    "trainer.train()\n",
    "print_memory_usage(\"After LoRA Training\")\n",
    "\n",
    "print(f\"Saving LoRA model to {output_dir}...\")\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"‚úÖ Done! LoRA model saved to Google Drive.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8810607,
     "sourceId": 13834204,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8810612,
     "sourceId": 13834210,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
