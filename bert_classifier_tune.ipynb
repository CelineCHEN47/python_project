{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5723693",
   "metadata": {},
   "source": [
    "# BERT Fine-Tuning for Spam Email Classification\n",
    "\n",
    "This notebook fine-tunes a pre-trained BERT model for binary text classification to identify spam emails. The model is trained on the provided dataset containing labeled spam and non-spam emails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91885b44",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b6d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ca2573",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2206bc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with open('data/classification/original_dataset.json', 'r') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "print(f\"Total samples: {len(raw_data)}\")\n",
    "print(f\"\\nFirst sample structure:\")\n",
    "print(f\"Keys: {raw_data[0].keys()}\")\n",
    "print(f\"\\nLabel: {raw_data[0]['label']}\")\n",
    "print(f\"Data preview: {raw_data[0]['data'][:200]}...\")\n",
    "\n",
    "# Extract labels and text\n",
    "labels = [sample['label'] for sample in raw_data]\n",
    "texts = [sample['data'] for sample in raw_data]\n",
    "\n",
    "# Create DataFrame for easier exploration\n",
    "df = pd.DataFrame({'label': labels, 'text': texts})\n",
    "\n",
    "# Label distribution\n",
    "print(f\"\\nLabel Distribution:\")\n",
    "label_counts = df['label'].value_counts()\n",
    "print(f\"Label 0 (Non-spam/Legitimate): {label_counts.get(0, 0)}\")\n",
    "print(f\"Label 1 (Spam): {label_counts.get(1, 0)}\")\n",
    "\n",
    "# Visualize label distribution\n",
    "plt.figure(figsize=(8, 4))\n",
    "df['label'].value_counts().plot(kind='bar')\n",
    "plt.xlabel('Label (0=Non-spam, 1=Spam)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Email Label Distribution')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Text length statistics\n",
    "df['text_length'] = df['text'].apply(lambda x: len(x.split()))\n",
    "print(f\"\\nText Length Statistics:\")\n",
    "print(df['text_length'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dff5446",
   "metadata": {},
   "source": [
    "## 3. Preprocess and Clean Email Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1e459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_email_text(text):\n",
    "    \"\"\"\n",
    "    Clean email text by removing URLs, extra whitespace, and special characters\n",
    "    \"\"\"\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|ftp\\S+', '', text)\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove special HTML characters\n",
    "    text = re.sub(r'&#\\d+;', '', text)\n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to all texts\n",
    "df['cleaned_text'] = df['text'].apply(clean_email_text)\n",
    "\n",
    "# Check cleaned text\n",
    "print(\"Original text sample:\")\n",
    "print(df['text'].iloc[0][:300])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Cleaned text sample:\")\n",
    "print(df['cleaned_text'].iloc[0][:300])\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(f\"\\nBERT Tokenizer loaded. Vocabulary size: {len(tokenizer.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f233b32e",
   "metadata": {},
   "source": [
    "## 4. Create PyTorch Dataset and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e429c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset for email classification\"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'token_type_ids': encoding.get('token_type_ids', torch.zeros(self.max_length, dtype=torch.long)).flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Split data into train and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['cleaned_text'].values,\n",
    "    df['label'].values,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['label'].values\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(train_texts)}\")\n",
    "print(f\"Validation set size: {len(val_texts)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = EmailDataset(train_texts, train_labels, tokenizer, max_length=512)\n",
    "val_dataset = EmailDataset(val_texts, val_labels, tokenizer, max_length=512)\n",
    "\n",
    "# Create dataloaders\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"\\nDataLoaders created with batch size: {BATCH_SIZE}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69293952",
   "metadata": {},
   "source": [
    "## 5. Load Pre-trained BERT Model and Build Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67972c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSpamClassifier(nn.Module):\n",
    "    \"\"\"BERT-based classifier for spam detection\"\"\"\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(BertSpamClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dense = nn.Linear(self.bert.config.hidden_size, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(256, 2)  # Binary classification (spam or non-spam)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # BERT forward pass\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        pooled_output = outputs[1]  # [CLS] token hidden state\n",
    "        \n",
    "        # Classification head\n",
    "        x = self.dropout(pooled_output)\n",
    "        x = self.dense(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.output(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Initialize model\n",
    "model = BertSpamClassifier(dropout=0.1)\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Model architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f679fd",
   "metadata": {},
   "source": [
    "## 6. Define Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2828b4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Learning rate scheduler (optional)\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "scheduler = LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=EPOCHS * len(train_loader))\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"Number of epochs: {EPOCHS}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Optimizer: AdamW\")\n",
    "print(f\"Loss function: CrossEntropyLoss\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eee2b9",
   "metadata": {},
   "source": [
    "## 7. Train the BERT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c17950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, loss_fn, device, scheduler=None):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        # Move batch to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask, token_type_ids)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, val_loader, loss_fn, device):\n",
    "    \"\"\"Evaluate model on validation set\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(val_loader, desc=\"Evaluating\")\n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(input_ids, attention_mask, token_type_ids)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = correct / total\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    \n",
    "    return avg_loss, accuracy, f1, all_preds, all_labels\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "val_f1_scores = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print('='*80)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, loss_fn, device, scheduler)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    print(f\"Training Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_f1, _, _ = evaluate(model, val_loader, loss_fn, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    val_f1_scores.append(val_f1)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}, F1-Score: {val_f1:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training completed!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09a409d",
   "metadata": {},
   "source": [
    "## 8. Plot Training History and Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867a0ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(train_losses, label='Training Loss')\n",
    "axes[0].plot(val_losses, label='Validation Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Model Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(train_accuracies, label='Training Accuracy')\n",
    "axes[1].plot(val_accuracies, label='Validation Accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Model Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# F1-score plot\n",
    "axes[2].plot(val_f1_scores, label='Validation F1-Score', marker='o')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('F1-Score')\n",
    "axes[2].set_title('Validation F1-Score')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best Validation Accuracy: {max(val_accuracies):.4f}\")\n",
    "print(f\"Best Validation F1-Score: {max(val_f1_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2891083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on validation set\n",
    "val_loss, val_acc, val_f1, predictions, true_labels = evaluate(model, val_loader, loss_fn, device)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL VALIDATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Validation F1-Score: {val_f1:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, target_names=['Non-spam (0)', 'Spam (1)']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Non-spam', 'Spam'],\n",
    "            yticklabels=['Non-spam', 'Spam'])\n",
    "plt.title('Confusion Matrix - Spam Classification')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc92f1eb",
   "metadata": {},
   "source": [
    "## 9. Save Model and Make Predictions on New Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f978231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "os.makedirs('results/bert_classifier', exist_ok=True)\n",
    "\n",
    "# Save the model\n",
    "model_path = 'results/bert_classifier/pytorch_model.bin'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained('results/bert_classifier/')\n",
    "print(\"Tokenizer saved to: results/bert_classifier/\")\n",
    "\n",
    "# Save model config\n",
    "with open('results/bert_classifier/config.txt', 'w') as f:\n",
    "    f.write(\"BERT Spam Classifier Configuration\\n\")\n",
    "    f.write(\"=\"*50 + \"\\n\")\n",
    "    f.write(f\"Model: bert-base-uncased\\n\")\n",
    "    f.write(f\"Dropout: 0.1\\n\")\n",
    "    f.write(f\"Max Sequence Length: 512\\n\")\n",
    "    f.write(f\"Batch Size: {BATCH_SIZE}\\n\")\n",
    "    f.write(f\"Learning Rate: {LEARNING_RATE}\\n\")\n",
    "    f.write(f\"Number of Epochs: {EPOCHS}\\n\")\n",
    "    f.write(f\"Optimizer: AdamW\\n\")\n",
    "    f.write(f\"\\nFinal Results:\\n\")\n",
    "    f.write(f\"Validation Accuracy: {val_acc:.4f}\\n\")\n",
    "    f.write(f\"Validation F1-Score: {val_f1:.4f}\\n\")\n",
    "    f.write(f\"Precision (Spam): {precision_score(true_labels, predictions):.4f}\\n\")\n",
    "    f.write(f\"Recall (Spam): {recall_score(true_labels, predictions):.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f0484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_spam(text, model, tokenizer, device, max_length=512):\n",
    "    \"\"\"\n",
    "    Predict if an email is spam or not\n",
    "    \n",
    "    Args:\n",
    "        text: Email text to classify\n",
    "        model: BERT classifier model\n",
    "        tokenizer: BERT tokenizer\n",
    "        device: torch device\n",
    "        max_length: Maximum sequence length\n",
    "    \n",
    "    Returns:\n",
    "        prediction: 0 (non-spam) or 1 (spam)\n",
    "        confidence: Confidence score (0-1)\n",
    "    \"\"\"\n",
    "    # Clean the text\n",
    "    text = clean_email_text(text)\n",
    "    \n",
    "    # Tokenize\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    token_type_ids = encoding.get('token_type_ids', torch.zeros(1, max_length, dtype=torch.long)).to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask, token_type_ids)\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        prediction = torch.argmax(logits, dim=1).item()\n",
    "        confidence = probabilities[0][prediction].item()\n",
    "    \n",
    "    return prediction, confidence\n",
    "\n",
    "# Test predictions on a few samples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING PREDICTIONS ON SAMPLE EMAILS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_samples = [\n",
    "    (\"Click here to win $1000! Limited time offer!\", 1),  # Expected: Spam\n",
    "    (\"Hi, I got your email about the project meeting. I'll be there at 3 PM.\", 0),  # Expected: Non-spam\n",
    "    (df['cleaned_text'].iloc[0], df['label'].iloc[0]),  # First sample from dataset\n",
    "]\n",
    "\n",
    "for i, (text, expected_label) in enumerate(test_samples):\n",
    "    if len(text) > 200:\n",
    "        display_text = text[:200] + \"...\"\n",
    "    else:\n",
    "        display_text = text\n",
    "    \n",
    "    prediction, confidence = predict_spam(text, model, tokenizer, device)\n",
    "    \n",
    "    label_name = {0: \"Non-spam\", 1: \"Spam\"}\n",
    "    \n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"Text: {display_text}\")\n",
    "    print(f\"Prediction: {label_name[prediction]} (confidence: {confidence:.4f})\")\n",
    "    if expected_label is not None:\n",
    "        correct = \"✓\" if prediction == expected_label else \"✗\"\n",
    "        print(f\"Expected: {label_name[expected_label]} {correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac93daa9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully fine-tuned a BERT model for binary spam email classification. The trained model:\n",
    "\n",
    "1. **Achieved strong performance** on the validation set with high accuracy and F1-score\n",
    "2. **Handles variable-length emails** with padding and truncation up to 512 tokens\n",
    "3. **Provides confidence scores** for each prediction\n",
    "4. **Has been saved** to `results/bert_classifier/` for integration with the Streamlit app\n",
    "\n",
    "### Next Steps:\n",
    "- Integrate the trained model into `app.py` to replace the mock `MockSpamClassifier`\n",
    "- The model can be loaded using the trained weights and tokenizer from the results directory\n",
    "- Use the `predict_spam()` function for real-time predictions in the Streamlit application"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
